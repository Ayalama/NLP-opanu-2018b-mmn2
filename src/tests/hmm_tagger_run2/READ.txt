test params:
############## train ###########
--model=bi-gram
--train_file=C:\\Users\\aymann\\PycharmProjects\\maman_12_NLP\\datasets\\heb-pos.train
--smoothing=n

############## decode ###########
--model=bi-gram
--test_file=C:\Users\aymann\PycharmProjects\maman_12_NLP\datasets\heb-pos.test
--param_file1=C:\Users\aymann\PycharmProjects\maman_12_NLP\tests\hmm_tagger_run2\hmm_tagger.lex
--param_file2=C:\Users\aymann\PycharmProjects\maman_12_NLP\tests\hmm_tagger_run2\hmm_tagger.gram

############## eval ###########
--model_tagged=C:\Users\aymann\PycharmProjects\maman_12_NLP\tests\hmm_tagger_run2\hmm_tagger.tagged
--gold_file=C:\Users\aymann\PycharmProjects\maman_12_NLP\datasets\heb-pos.gold
--model=bi-gram
--smoothing=n

############## Additional info ###########
* run without log prob transformation
* cases where set of possible states for a word w_i is empty and w_i is known , meaning for each state: p(w_i|s_i)=0 or p(s_i|s_i-1)=0:
    - possible states for transition are s where
    1. not included: (p(w_i|s_i)<>0 and p(s_i|s_i-1)=0). in this case val_si= max p(w_i|s_i)*val_s'(i-1) . prev state will be argmax s'
    2. (p(w_i|s_i)=0 and p(s_i|s_i-1)<>0). in this case val_si= max p(s_i|s_i-1)*val_s_i-1(i-1) . prev state will be argmax on s_i-1